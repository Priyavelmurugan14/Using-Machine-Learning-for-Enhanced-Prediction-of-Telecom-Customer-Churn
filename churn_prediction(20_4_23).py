# -*- coding: utf-8 -*-
"""churn-prediction(20-4-23).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iuz7VREJAP0NdoshrslbXaC_AEvD0B1w
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
from imblearn.over_sampling import SMOTE


# models
from sklearn.linear_model import LogisticRegression # LR
from sklearn.neighbors import KNeighborsClassifier # KNN
from sklearn.tree import DecisionTreeClassifier # DT
from sklearn.ensemble import RandomForestClassifier # RF
from xgboost import XGBClassifier # XGBM
from sklearn.ensemble import AdaBoostClassifier # AdaBoost
from  lightgbm import LGBMClassifier # Light GBM


# evaluation
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

sns.set_palette('dark')
sns.set_style('darkgrid')

df = pd.read_csv("../content/drive/MyDrive/Churn_Modelling.csv")
df.head()

df = df.drop(columns=['RowNumber', 'CustomerId', 'Surname'])

df.info()

df.describe()

density = df['Exited'].value_counts(normalize=True).reset_index()
sns.barplot(data=density, x='index', y='Exited', );
density

categorical = df.drop(columns=['CreditScore', 'Age', 'Tenure', 'Balance', 'EstimatedSalary'])
rows = int(np.ceil(categorical.shape[1] / 2)) - 1

# create sub-plots anf title them
fig, axes = plt.subplots(nrows=rows, ncols=2, figsize=(10,6))
axes = axes.flatten()

for row in range(rows):
    cols = min(2, categorical.shape[1] - row*2)
    for col in range(cols):
        col_name = categorical.columns[2 * row + col]
        ax = axes[row*2 + col]       

        sns.countplot(data=categorical, x=col_name, hue="Exited", ax=ax);
        
plt.tight_layout()

def box_scatter(data, x, y):    
    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(16,6))
    sns.boxplot(data=data, x=x, ax=ax1)
    sns.scatterplot(data=data, x=x,y=y,ax=ax2)

box_scatter(df,'CreditScore','Exited');
plt.tight_layout()
print(f"# of Bivariate Outliers: {len(df.loc[df['CreditScore'] < 400])}")

X = df.drop(columns=['Exited'])
y = df['Exited']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)

def untuned(X_train, X_test, y_train, y_test):
    lr = LogisticRegression()
    knn = KNeighborsClassifier()
    dt = DecisionTreeClassifier()
    rf = RandomForestClassifier()
    ada = AdaBoostClassifier()
    lgb = LGBMClassifier()
    xgb = XGBClassifier()
   
    
    names = ['Linear Regression', 'KNN', 'DT', 'RF', 'AdaBoost', 'Light GBM', 'XGBoost']
    algos = [lr, knn, dt, rf, ada, lgb, xgb]
    acc = [None] * 7
    precision = [None] * 7
    recall = [None] * 7
    f1 = [None] * 7
    
    for indx, model in enumerate(algos):
        try:
            if(indx < 7):
                model.fit(X_train, y_train)
            else:
                model.fit(X_train, y_train, silent=True)
                
            y_pred = model.predict(X_test)
            acc[indx] = model.score(X_test, y_test)            
            precision[indx] = precision_score(y_test, y_pred)
            recall[indx] = recall_score(y_test, y_pred)
            f1[indx] = f1_score(y_test, y_pred)
            
        except Exception as e:
            acc[indx] = "-"
            precision[indx] = "-"
            recall[indx] = "-"
            f1[indx] = "-"
            print(f"{names[indx]}: {str(e)}")
        
    return pd.DataFrame([acc, precision, recall, f1],
                        index=['Accuracy', 'Precision', 'Recall', 'F1'],columns=names).T

nothing = untuned(X_train, X_test, y_train, y_test)
nothing

encoding = untuned(X_train, X_test, y_train, y_test)
encoding

ss = StandardScaler()
X = ss.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)

scaling = untuned(X_train, X_test, y_train, y_test)
scaling

sm = SMOTE(random_state=42)
X, y = sm.fit_resample(X,y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)

imbalance = untuned(X_train, X_test, y_train, y_test)
imbalance



